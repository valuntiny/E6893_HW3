{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Guojing Wu** | UNI: gw2383 | *2019-10-28*\n",
    "\n",
    "# E6893 BIg Data Analytics Homework3\n",
    "\n",
    "## Setup\n",
    "\n",
    "use `twitterHTTPClient.py` to connect to Twitter\n",
    "\n",
    "create a dataset in BigQuery called `bigdata_sparkStreaming`\n",
    "\n",
    "## streaming fetching\n",
    "\n",
    "below is the code for tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Time: 2019-11-13 17:00:25\n",
      "-------------------------------------------\n",
      "('movie', 279, datetime.datetime(2019, 11, 13, 17, 0, 25))\n",
      "('ai', 25, datetime.datetime(2019, 11, 13, 17, 0, 25))\n",
      "('good', 8, datetime.datetime(2019, 11, 13, 17, 0, 25))\n",
      "('spark', 11, datetime.datetime(2019, 11, 13, 17, 0, 25))\n",
      "('data', 9, datetime.datetime(2019, 11, 13, 17, 0, 25))\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2019-11-13 17:01:25\n",
      "-------------------------------------------\n",
      "('movie', 282, datetime.datetime(2019, 11, 13, 17, 1, 25))\n",
      "('good', 12, datetime.datetime(2019, 11, 13, 17, 1, 25))\n",
      "('ai', 23, datetime.datetime(2019, 11, 13, 17, 1, 25))\n",
      "('spark', 11, datetime.datetime(2019, 11, 13, 17, 1, 25))\n",
      "('data', 14, datetime.datetime(2019, 11, 13, 17, 1, 25))\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2019-11-13 17:02:25\n",
      "-------------------------------------------\n",
      "('movie', 290, datetime.datetime(2019, 11, 13, 17, 2, 25))\n",
      "('ai', 28, datetime.datetime(2019, 11, 13, 17, 2, 25))\n",
      "('good', 5, datetime.datetime(2019, 11, 13, 17, 2, 25))\n",
      "('spark', 7, datetime.datetime(2019, 11, 13, 17, 2, 25))\n",
      "('data', 7, datetime.datetime(2019, 11, 13, 17, 2, 25))\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2019-11-13 17:03:25\n",
      "-------------------------------------------\n",
      "('movie', 285, datetime.datetime(2019, 11, 13, 17, 3, 25))\n",
      "('ai', 17, datetime.datetime(2019, 11, 13, 17, 3, 25))\n",
      "('good', 10, datetime.datetime(2019, 11, 13, 17, 3, 25))\n",
      "('spark', 8, datetime.datetime(2019, 11, 13, 17, 3, 25))\n",
      "('data', 8, datetime.datetime(2019, 11, 13, 17, 3, 25))\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2019-11-13 17:04:25\n",
      "-------------------------------------------\n",
      "('movie', 281, datetime.datetime(2019, 11, 13, 17, 4, 25))\n",
      "('good', 8, datetime.datetime(2019, 11, 13, 17, 4, 25))\n",
      "('ai', 24, datetime.datetime(2019, 11, 13, 17, 4, 25))\n",
      "('spark', 8, datetime.datetime(2019, 11, 13, 17, 4, 25))\n",
      "('data', 3, datetime.datetime(2019, 11, 13, 17, 4, 25))\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2019-11-13 17:05:25\n",
      "-------------------------------------------\n",
      "('movie', 280, datetime.datetime(2019, 11, 13, 17, 5, 25))\n",
      "('ai', 24, datetime.datetime(2019, 11, 13, 17, 5, 25))\n",
      "('good', 12, datetime.datetime(2019, 11, 13, 17, 5, 25))\n",
      "('spark', 5, datetime.datetime(2019, 11, 13, 17, 5, 25))\n",
      "('data', 5, datetime.datetime(2019, 11, 13, 17, 5, 25))\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2019-11-13 17:06:25\n",
      "-------------------------------------------\n",
      "('movie', 287, datetime.datetime(2019, 11, 13, 17, 6, 25))\n",
      "('ai', 33, datetime.datetime(2019, 11, 13, 17, 6, 25))\n",
      "('good', 7, datetime.datetime(2019, 11, 13, 17, 6, 25))\n",
      "('spark', 11, datetime.datetime(2019, 11, 13, 17, 6, 25))\n",
      "('data', 6, datetime.datetime(2019, 11, 13, 17, 6, 25))\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2019-11-13 17:07:25\n",
      "-------------------------------------------\n",
      "('movie', 301, datetime.datetime(2019, 11, 13, 17, 7, 25))\n",
      "('ai', 22, datetime.datetime(2019, 11, 13, 17, 7, 25))\n",
      "('good', 15, datetime.datetime(2019, 11, 13, 17, 7, 25))\n",
      "('data', 6, datetime.datetime(2019, 11, 13, 17, 7, 25))\n",
      "('spark', 5, datetime.datetime(2019, 11, 13, 17, 7, 25))\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2019-11-13 17:08:25\n",
      "-------------------------------------------\n",
      "('ai', 18, datetime.datetime(2019, 11, 13, 17, 8, 25))\n",
      "('movie', 276, datetime.datetime(2019, 11, 13, 17, 8, 25))\n",
      "('good', 7, datetime.datetime(2019, 11, 13, 17, 8, 25))\n",
      "('data', 7, datetime.datetime(2019, 11, 13, 17, 8, 25))\n",
      "('spark', 6, datetime.datetime(2019, 11, 13, 17, 8, 25))\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2019-11-13 17:10:25\n",
      "-------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "This module is the spark streaming analysis process.\n",
    "\n",
    "Todo:\n",
    "    1. hashtagCount: calculate accumulated hashtags count\n",
    "    2. wordCount: calculate word count every 60 seconds\n",
    "        the word you should track is listed below.\n",
    "    3. save the result to google BigQuery\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "from pyspark import SparkConf,SparkContext\n",
    "from pyspark.streaming import StreamingContext\n",
    "from pyspark.sql import Row,SQLContext\n",
    "import sys\n",
    "import requests\n",
    "import subprocess\n",
    "from google.cloud import bigquery\n",
    "import re\n",
    "from pyspark.sql.functions import lit, unix_timestamp\n",
    "import time\n",
    "import datetime\n",
    "\n",
    "# parameter\n",
    "IP = 'localhost'    # ip port\n",
    "PORT = 9001       # port\n",
    "STREAMTIME = 600          # time that the streaming process runs\n",
    "windowLength = 60        # window length for wordcount\n",
    "slideInterval = 60        # slide interval for wordcount\n",
    "timestamp = datetime.datetime.fromtimestamp(time.time()).strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "\n",
    "# global variables\n",
    "bucket = \"big_data_hw\"    # TODO : replace with your own bucket name\n",
    "output_directory_hashtags = 'gs://{}/hadoop/tmp/bigquery/pyspark_output/hashtagsCount'.format(bucket)\n",
    "output_directory_wordcount = 'gs://{}/hadoop/tmp/bigquery/pyspark_output/wordcount'.format(bucket)\n",
    "\n",
    "# output table and columns name\n",
    "output_dataset = 'bigdata_sparkStreaming'        #the name of your dataset in BigQuery\n",
    "output_table_hashtags = 'hashtags'\n",
    "columns_name_hashtags = ['hashtags', 'count']\n",
    "output_table_wordcount = 'wordcount'\n",
    "columns_name_wordcount = ['word', 'count', 'time']\n",
    "\n",
    "WORD = ['data', 'spark', 'ai', 'movie', 'good']     #the words you should filter and do word count\n",
    "\n",
    "def saveToBigQuery(sc, output_dataset, output_table, directory):\n",
    "    \"\"\"\n",
    "    Put temp streaming json files in google storage to google BigQuery\n",
    "    and clean the output files in google storage\n",
    "    \"\"\"\n",
    "    files = directory + '/part-*'\n",
    "    subprocess.check_call(\n",
    "        'bq load --source_format NEWLINE_DELIMITED_JSON '\n",
    "        '--replace '\n",
    "        '--autodetect '\n",
    "        '{dataset}.{table} {files}'.format(\n",
    "            dataset=output_dataset, table=output_table, files=files\n",
    "        ).split())\n",
    "    output_path = sc._jvm.org.apache.hadoop.fs.Path(directory)\n",
    "    output_path.getFileSystem(sc._jsc.hadoopConfiguration()).delete(\n",
    "        output_path, True)\n",
    "\n",
    "def saveToStorage_hash(rdd):\n",
    "    \"\"\"\n",
    "    Save each RDD in this DStream to google storage\n",
    "    Args:\n",
    "        rdd: input rdd\n",
    "        output_directory: output directory in google storage\n",
    "        columns_name: columns name of dataframe\n",
    "        mode: mode = \"overwirte\", overwirte the file\n",
    "              mode = \"append\", append data to the end of file\n",
    "    \"\"\"\n",
    "    if not rdd.isEmpty():\n",
    "        rdd.toDF( columns_name_hashtags ) \\\n",
    "        .orderBy('count', ascending=False) \\\n",
    "        .write.save(output_directory_hashtags, format=\"json\", mode=\"overwrite\")\n",
    "        \n",
    "def saveToStorage_word(rdd):\n",
    "    \"\"\"\n",
    "    Save each RDD in this DStream to google storage\n",
    "    Args:\n",
    "        rdd: input rdd\n",
    "        output_directory: output directory in google storage\n",
    "        columns_name: columns name of dataframe\n",
    "        mode: mode = \"overwirte\", overwirte the file\n",
    "              mode = \"append\", append data to the end of file\n",
    "    \"\"\"\n",
    "    if not rdd.isEmpty():\n",
    "        rdd.toDF( columns_name_wordcount ) \\\n",
    "        .orderBy(['word']) \\\n",
    "        .write.save(output_directory_wordcount, format=\"json\", mode=\"append\")\n",
    "\n",
    "# helper function\n",
    "def filterFunc(hashtags):\n",
    "    if re.match(\"^#[0-9a-z]+$\", hashtags):\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "# helper function\n",
    "def updateFunction(newValues, runningCount):\n",
    "    if runningCount is None:\n",
    "        runningCount = 0\n",
    "    return sum(newValues, runningCount)  # add the new values with the previous running count to get the new count\n",
    "\n",
    "def hashtagCount(words):\n",
    "    \"\"\"\n",
    "    Calculate the accumulated hashtags count sum from the beginning of the stream\n",
    "    and sort it by descending order of the count.\n",
    "    Ignore case sensitivity when counting the hashtags:\n",
    "        \"#Ab\" and \"#ab\" is considered to be a same hashtag\n",
    "    You have to:\n",
    "    1. Filter out the word that is hashtags.\n",
    "       Hashtag usually start with \"#\" and followed by a serious of alphanumeric\n",
    "    2. map (hashtag) to (hashtag, 1)\n",
    "    3. sum the count of current DStream state and previous state\n",
    "    4. transform unordered DStream to a ordered Dstream\n",
    "    Hints:\n",
    "        you may use regular expression to filter the words\n",
    "        You can take a look at updateStateByKey and transform transformations\n",
    "    Args:\n",
    "        dstream(DStream): stream of real time tweets\n",
    "    Returns:\n",
    "        DStream Object with inner structure (hashtag, count)\n",
    "    \"\"\"\n",
    "    \n",
    "    hashtags = words \\\n",
    "    .map(lambda x: x.lower()) \\\n",
    "    .filter(filterFunc) \\\n",
    "    .map(lambda x: (x, 1)) \\\n",
    "    .updateStateByKey(updateFunction) \\\n",
    "    .transform(lambda rdd: rdd.sortBy(lambda x: x[1], ascending=False))\n",
    "    \n",
    "    return hashtags\n",
    "\n",
    "def wordCount(words):\n",
    "    \"\"\"\n",
    "    Calculte the count of 5 sepcial words in 60 seconds for every 60 seconds (window no overlap)\n",
    "    You can choose your own words.\n",
    "    Your should:\n",
    "    1. filter the words\n",
    "    2. count the word during a special window size\n",
    "    3. add a time related mark to the output of each window, ex: a datetime type\n",
    "    Hints:\n",
    "        You can take a look at reduceByKeyAndWindow transformation\n",
    "        Dstream is a serious of rdd, each RDD in a DStream contains data from a certain interval\n",
    "        You may want to take a look of transform transformation of DStream when trying to add a time\n",
    "    Args:\n",
    "        dstream(DStream): stream of real time tweets\n",
    "    Returns:\n",
    "        DStream Object with inner structure (word, count, time)\n",
    "    \"\"\"\n",
    "\n",
    "    res = words \\\n",
    "    .map(lambda x: x.lower()) \\\n",
    "    .filter(lambda x: x in WORD) \\\n",
    "    .map(lambda x: (x, 1)) \\\n",
    "    .reduceByKeyAndWindow(lambda x, y: x+y, lambda x, y: x-y, windowLength, slideInterval) \\\n",
    "    .transform(lambda timestamp, rdd: rdd.map(lambda x: (x[0], x[1], timestamp)))\n",
    "    \n",
    "    return res\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # Spark settings\n",
    "    conf = SparkConf()\n",
    "    conf.setMaster('local[2]')\n",
    "    conf.setAppName(\"TwitterStreamApp\")\n",
    "\n",
    "    # create spark context with the above configuration\n",
    "    sc = SparkContext.getOrCreate(conf=conf)\n",
    "    sc.setLogLevel(\"ERROR\")\n",
    "\n",
    "    # create sql context, used for saving rdd\n",
    "    sql_context = SQLContext(sc)\n",
    "\n",
    "    # create the Streaming Context from the above spark context with batch interval size 5 seconds\n",
    "    ssc = StreamingContext(sc, 5)\n",
    "    # setting a checkpoint to allow RDD recovery\n",
    "    ssc.checkpoint(\"~/checkpoint_TwitterApp\")\n",
    "\n",
    "    # read data from port 9001\n",
    "    dataStream = ssc.socketTextStream(IP, PORT)\n",
    "    words = dataStream.flatMap(lambda line: line.split(\" \"))\n",
    "\n",
    "    # calculate the accumulated hashtags count sum from the beginning of the stream\n",
    "#     topTags = hashtagCount(words)\n",
    "#     topTags.pprint()\n",
    "    \n",
    "    # Calculte the word count during each time period 60s\n",
    "    wordCount = wordCount(words)\n",
    "    wordCount.pprint()\n",
    "\n",
    "#     topTags.foreachRDD(saveToStorage_hash)\n",
    "    wordCount.foreachRDD(saveToStorage_word)\n",
    "\n",
    "    # start streaming process, wait for 600s and then stop.\n",
    "    ssc.start()\n",
    "    time.sleep(STREAMTIME)\n",
    "    ssc.stop(stopSparkContext=False, stopGraceFully=True)\n",
    "    \n",
    "#     saveToBigQuery(sc, output_dataset, output_table_hashtags, output_directory_hashtags)\n",
    "    saveToBigQuery(sc, output_dataset, output_table_wordcount, output_directory_wordcount)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result will be stored in BigQuery"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
